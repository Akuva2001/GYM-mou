{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec422880",
   "metadata": {},
   "source": [
    "# Optimal control course project\n",
    "# Solving the CartPole-v1 problem\n",
    "\n",
    "by **Artem Petrov** and **Ivan Kudriakov**\n",
    "\n",
    "Solving CartPole-v1 from openai.gym using methods of the Optimal Control Theory\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5434e7da-5254-4fa2-b41a-b90567318276",
   "metadata": {},
   "source": [
    "# Step 1 \n",
    "### (March 9, 2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf8682a",
   "metadata": {},
   "source": [
    "## Mathematical model\n",
    "\n",
    "The environment simulation of CartPole-v1 is performed according to the model, described here: https://coneural.org/florian/papers/05_cart_pole.pdf\n",
    "\n",
    "([Proof from the code of CartPole-v1](https://github.com/openai/gym/blob/2dddaf722acccfd0412d745890c40dcd972586d5/gym/envs/classic_control/cartpole.py#L126))\n",
    "\n",
    "Here, I will provide the brief explanation of the model\n",
    "\n",
    "<img src=\"presentation_media/forces_model.png\" alt=\"drawing\" width=\"500\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792e0f4f",
   "metadata": {},
   "source": [
    "<!-- Here $G_c$ and $G_p$ are the forces of gravity acting on cart and pole respectively.\n",
    "\n",
    "Here $N_c$ - reaction force and $F_f$ - friction force between the cart and the rail -->\n",
    "\n",
    "Original model considers the friction forces, however in CartPole-v1 there are no friction.\n",
    "\n",
    "<!-- \n",
    "$\\mu_c$ - friction coefficient between the cart and the rail\n",
    "\n",
    "$\\mu_p$ - friction coefficient between the cart and the pole\n",
    "\n",
    "Side note: the initial model assumes Lubricated friction between the pole and the cart which is somewhat counterintuitive. -->\n",
    "\n",
    "From the frictionless Newton equations the following  movement equations can be derived:\n",
    "\n",
    "$$\n",
    "\\ddot{\\theta} = \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- F - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\n",
    "\\label{eq:theta} \\tag{1}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\ddot{x} = \\frac{F + m_p l\\left(\\dot{\\theta}^2 \\sin \\theta - \\ddot{\\theta}\\cos\\theta\\right)}\n",
    "{m_c + m_p}\n",
    "\\label{eq:x} \\tag{2}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "$m_c$ - the mass of the cart \n",
    "\n",
    "$m_p$ - the mass of the pole \n",
    "\n",
    "$l$ - half-length of the pole\n",
    "\n",
    "$F = \\pm F_0$ - horizontal force applied to the cart by the agent.\n",
    "\n",
    "Interested reader who want to check out the whole inference of the equations above may want to read [the original source](https://coneural.org/florian/papers/05_cart_pole.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1cefcc",
   "metadata": {},
   "source": [
    "#### Now, let's pose the problem in terms of the optimal control theory.\n",
    "\n",
    "The goal is to keep the pole upright for the longest time. \n",
    "\n",
    "Hence, one might suggests the usage of one the following simple functionals:\n",
    "\n",
    "$$ \n",
    "\\left[\n",
    "\\begin{array}[lll]\n",
    "    .\n",
    "    J_{\\text{1}} = \\int\\limits_{0}^{\\infty} \\theta^2 dt \\rightarrow \\min \\\\\n",
    "    J_{\\text{2}} = \\int\\limits_{0}^{T} \\theta^2 dt \\rightarrow \\min \\\\\n",
    "    J_{\\text{3}} = \\int\\limits_{0}^{T} |\\theta| dt \\rightarrow \\min\n",
    "\\end{array}\n",
    "\\right .\n",
    "$$ \n",
    "\n",
    "\n",
    "However, if we want to optimize for the highest value of the agent's reward function, the following functional must be used:\n",
    "\n",
    "$$ J = T \\rightarrow \\max \\tag{3} $$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let $\\mathbf{q} = (x, \\dot{x}, \\theta, \\dot{\\theta})^T$\n",
    "\n",
    "$F = u F_0$, where $u \\in \\{-1, +1\\}$ - control\n",
    "\n",
    "Then, the dynamic constaraints can be written as follows:\n",
    "\n",
    "$$\\mathbf{\\dot{q}} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\ddot{x} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\ddot{\\theta}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\frac{F + m_p l\\left(\\dot{\\theta}^2 \\sin \\theta - \\ddot{\\theta}\\cos\\theta\\right)}{m_c + m_p} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- F - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\frac{uF_0 + m_p l\\left(\\dot{\\theta}^2 \\sin \\theta - \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- uF_0 - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\\cos\\theta\\right)}{m_c + m_p} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- u F_0 - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\n",
    "\\end{bmatrix} = \n",
    "\\mathbf{f}(\\mathbf{q}, u)$$\n",
    "\n",
    "\n",
    "\n",
    "Also, we must consider the fact, that in this particular implementation, the simulation will stop if one of the following conditions is reached:\n",
    "\n",
    "- $|\\theta| > 12^\\circ$  (which contradicts the [frontpage](https://gym.openai.com/envs/CartPole-v1/), but is consistent with the [actual code](https://github.com/openai/gym/blob/2dddaf722acccfd0412d745890c40dcd972586d5/gym/envs/classic_control/cartpole.py#L90))\n",
    "\n",
    "- $|x| > 2.4$\n",
    "\n",
    "- $ t \\le 500\\tau$, where $\\tau = 0.02$sec - is the length of one episode of simulation\n",
    "\n",
    "These conditions can be formulated as the path constraints:\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "    |q_1| = |x| \\le 2.4\\\\\n",
    "    |q_3| = |\\theta| \\le 12^\\circ \\\\\n",
    "    t \\le 500\\tau\n",
    "\\end{cases} \\tag{4}\n",
    "$$\n",
    "\n",
    "However, because we maximize the time when our trajectory stays within the boundaries of \n",
    "$\\begin{cases}\n",
    "    |q_1| = |x| \\le 2.4\\\\\n",
    "    |q_3| = |\\theta| \\le 12^\\circ\n",
    "\\end{cases}$, the last condition in (4) can be dropped, as the subtrajectory $\\{\\mathbf{q}(t)| t \\in [0,500\\tau]\\}$ of a valid trajectory  $\\{\\mathbf{q}(t) | t \\in [0,T]; T>500\\tau\\}$ will be the optimal solution for this problem/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b77564",
   "metadata": {},
   "source": [
    "#### Parameters of the environment\n",
    "\n",
    "In order to get the model's parameters one can look at [the source code](https://github.com/openai/gym/blob/2dddaf722acccfd0412d745890c40dcd972586d5/gym/envs/classic_control/cartpole.py#L80) of the CartPole-v1 environment.\n",
    "\n",
    "$g = 9.8$\n",
    "\n",
    "$m_c = 1.0$ \n",
    "\n",
    "$m_p = 0.1$\n",
    "\n",
    "$l = 0.5$ \n",
    "\n",
    "$F_0 = 10.0$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682bb07-c9cd-4ce1-b778-1e0bdc70c717",
   "metadata": {},
   "source": [
    "# Step 2\n",
    "### (March 16 2022)\n",
    "\n",
    "# LQR solution\n",
    "\n",
    "## What is LQR (Linear–quadratic regulator)?\n",
    "\n",
    "### Reference information (by [Ru Wiki LQR](https://ru.wikipedia.org/wiki/%D0%9B%D0%B8%D0%BD%D0%B5%D0%B9%D0%BD%D0%BE-%D0%BA%D0%B2%D0%B0%D0%B4%D1%80%D0%B0%D1%82%D0%B8%D1%87%D0%BD%D1%8B%D0%B9_%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%82%D0%BE%D1%80), [En Wiki LQR](https://en.wikipedia.org/wiki/Linear%E2%80%93quadratic_regulator))\n",
    "\n",
    "If \n",
    "$$\\dot{x} = A(t)x + B(t)u$$ \n",
    "and \n",
    "$$J = \\int\\limits_{0}^\\infty \\left( x^T Q(t) x + u^T R(t) u \\right) dt$$ \n",
    "then optimal control is \n",
    "$$u = -R^{-1} B^T P x$$ \n",
    "when $P$ is solution of Ricatti equation: \n",
    "$$A^T P + P A - P B R^{-1} B^T P + Q = -\\dot{P}$$\n",
    "\n",
    "### At the last seminar we found:\n",
    "\n",
    "$$\\mathbf{\\dot{q}} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\ddot{x} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\ddot{\\theta}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\frac{F + m_p l\\left(\\dot{\\theta}^2 \\sin \\theta - \\ddot{\\theta}\\cos\\theta\\right)}{m_c + m_p} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- F - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\frac{uF_0 + m_p l\\left(\\dot{\\theta}^2 \\sin \\theta - \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- uF_0 - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\\cos\\theta\\right)}{m_c + m_p} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\frac{g \\sin \\theta + \\cos \\theta \\left( \\frac{- u F_0 - m_p l \\dot{\\theta}^2\\sin \\theta}{m_c + m_p} \\right)}\n",
    "{l\\left(\\frac{4}{3} - \\frac{m_p \\cos ^2 \\theta}{m_c + m_p}\\right)}\n",
    "\\end{bmatrix} = \n",
    "\\mathbf{f}(\\mathbf{q}, u)$$\n",
    "\n",
    "### Linearization around the upper equilibrium of our system:\n",
    "$$\\mathbf{\\dot{q}} = \n",
    "\\begin{bmatrix}\n",
    "    \\dot{x} \\\\\n",
    "    \\ddot{x} \\\\\n",
    "    \\dot{\\theta} \\\\\n",
    "    \\ddot{\\theta}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    0 & 1 & 0 & 0 \\\\\n",
    "    0 & 0 & \\frac{g}{l(\\frac{4}{3} - \\frac{m_p}{m_p+m_c})} & 0 \\\\\n",
    "    0 & 0 & 0 & 1 \\\\\n",
    "    0 & 0 & \\frac{g}{l(\\frac{4}{3} - \\frac{m_p}{m_p+m_c})} & 0 \n",
    "\\end{bmatrix}q + \n",
    "\\begin{bmatrix}\n",
    "    0 \\\\\n",
    "    \\frac{1}{m_p+m_c} \\\\\n",
    "    0 \\\\\n",
    "    -\\frac{1}{l(\\frac{4}{3} - \\frac{m_p}{m_p+m_c})}\n",
    "\\end{bmatrix}u $$\n",
    "\n",
    "\n",
    "$$\\mathbf{\\dot{q}} = A\\mathbf{q}+B\\mathbf{u}$$\n",
    "\n",
    "$$J = \\int\\limits_{0}^\\infty \\left( q^T Q q + u^T R u \\right) dt$$\n",
    "\n",
    "$$\\mathbf{u} = K \\mathbf{q} =  -R^{-1} B^T P \\mathbf{q}$$\n",
    "#### It means, in LQR control $\\mathbf{u}$ is linear function of $\\mathbf{q}$, but in inital problem it is discrete\n",
    "But if you look at the formula, you can see that when constrained to the maximum force, the parameter $R$ conditions the proximity of the selection function $\\mathbf{u}$ to $sign$.\n",
    "\n",
    "#### Code below refers to [Optimal Control with OpenAI Gym](https://towardsdatascience.com/comparing-optimal-control-and-reinforcement-learning-using-the-cart-pole-swing-up-openai-gym-772636bc48f4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f49910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "g = 9.8\n",
    "lp = 0.5\n",
    "mp = 0.1\n",
    "mc = 1.0\n",
    "mt = mp + mc # m_total\n",
    "\n",
    "\n",
    "# state matrix\n",
    "a = g/(lp*(4.0/3 - mp/mt))\n",
    "A = np.array([[0, 1, 0, 0],\n",
    "              [0, 0, a, 0],\n",
    "              [0, 0, 0, 1],\n",
    "              [0, 0, a, 0]])\n",
    "\n",
    "# input matrix\n",
    "b = -1/(lp*(4.0/3 - mp/mt))\n",
    "B = np.array([[0], [1/mt], [0], [b]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00c09153",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R = [[1]]\n",
      "Q =\n",
      " [[5 0 0 0]\n",
      " [0 5 0 0]\n",
      " [0 0 5 0]\n",
      " [0 0 0 5]]\n"
     ]
    }
   ],
   "source": [
    "R = np.eye(1, dtype=int)          # choose R (weight for input)\n",
    "print('R =', R)\n",
    "Q = 5*np.eye(4, dtype=int)        # choose Q (weight for state)\n",
    "print('Q =\\n', Q)\n",
    "\n",
    "# get riccati solver\n",
    "from scipy import linalg\n",
    "\n",
    "# solve ricatti equation\n",
    "P = linalg.solve_continuous_are(A, B, Q, R)\n",
    "\n",
    "# calculate optimal controller gain\n",
    "K = np.dot(np.linalg.inv(R),\n",
    "           np.dot(B.T, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5be9bfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_state_controller(K, x):\n",
    "    # feedback controller\n",
    "    u = -np.dot(K, x)   # u = -Kx\n",
    "    if u > 0:\n",
    "        return 1, u     # if force_dem > 0 -> move cart right\n",
    "    else:\n",
    "        return 0, u     # if force_dem <= 0 -> move cart left"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9a155b-b99c-46c8-b9d1-e354291fcd66",
   "metadata": {},
   "source": [
    "### In code below absolute value of force is changing on every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "491e7690-91a7-4e39-9ed4-f3d38f00081b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-14 21:58:58.564 Python[4408:186251] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/1b/hpr4ztpn02947rpwmgc0gzph0000gn/T/com.apple.python3.savedState\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated after 200 iterations.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# get environment\n",
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(500):\n",
    "    env.render()\n",
    "    \n",
    "    # get force direction (action) and force value (force)\n",
    "    action, force = apply_state_controller(K, obs)\n",
    "    \n",
    "    # absolute value, since 'action' determines the sign, F_min = -10N, F_max = 10N\n",
    "    abs_force = abs(float(np.clip(force, -10, 10)))\n",
    "    \n",
    "    # change magnitute of the applied force in CartPole\n",
    "    env.env.force_mag = abs_force\n",
    "\n",
    "    # apply action\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print(f'Terminated after {i+1} iterations.')\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cdafd7-cf6b-450d-a294-9a7a3bf17c5f",
   "metadata": {},
   "source": [
    "### Initially absolute value of force should be constant, so lets test LQR result with constant force"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6b6a4040-fe9c-4982-a482-ea7fc637307b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Terminated after 200 iterations.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# get environment\n",
    "env = gym.make('CartPole-v0')\n",
    "obs = env.reset()\n",
    "\n",
    "for i in range(500):\n",
    "    env.render()\n",
    "    \n",
    "    # get force direction (action) and force value (force)\n",
    "    action, force = apply_state_controller(K, obs)\n",
    "    \n",
    "    # absolute value, since 'action' determines the sign, F_min = -10N, F_max = 10N\n",
    "    abs_force = abs(float(np.clip(force, -10, 10)))\n",
    "    \n",
    "    # change magnitute of the applied force in CartPole\n",
    "    #env.env.force_mag = abs_force\n",
    "\n",
    "    # apply action\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "    if done:\n",
    "        print(f'Terminated after {i+1} iterations.')\n",
    "        break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182186a3-2db3-48bf-aefe-406c2e30baa9",
   "metadata": {},
   "source": [
    "# PID (proportional–integral–derivative controller)\n",
    "#### Reference by Wiki ([EN](https://en.wikipedia.org/wiki/PID_controller), [RU](https://ru.wikipedia.org/wiki/%D0%9F%D0%98%D0%94-%D1%80%D0%B5%D0%B3%D1%83%D0%BB%D1%8F%D1%82%D0%BE%D1%80))\n",
    "\n",
    "A proportional–integral–derivative controller (PID controller or three-term controller) is a control loop mechanism employing feedback that is widely used in industrial control systems and a variety of other applications requiring continuously modulated control. A PID controller continuously calculates an error value $e(t)$ as the difference between a desired setpoint (SP) and a measured process variable (PV) and applies a correction based on proportional, integral, and derivative terms (denoted P, I, and D respectively), hence the name.\n",
    "\n",
    "\n",
    "$$u(t) = K_\\text{p} e(t) + K_\\text{i} \\int_0^t e(\\tau) \\,\\mathrm{d}\\tau + K_\\text{d} \\frac{\\mathrm{d}e(t)}{\\mathrm{d}t}$$\n",
    "\n",
    "#### Cart-Pole problem\n",
    "Target value is $\\theta=0$\n",
    "$$u(t) = P\\theta + I\\int \\theta + D \\dot{\\theta}$$\n",
    "\n",
    "In our model we use sign of original control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbb0299c-c984-4bf0-a4f9-c9f36f4061b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "P, I, D = 1, 1, 1\n",
    "tau = 0.02\n",
    "\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    integral = 0\n",
    "    derivative = 0\n",
    "    prev_error = 0\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        integral += state[2] * tau\n",
    "        derivative = state[3]\n",
    "        error = state[2]\n",
    "        res = I * integral + D * derivative + P * error\n",
    "        if res>0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a599d12-99fc-40da-a199-3bc45eb0e380",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n",
      "Episode finished after 500 timesteps\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "\n",
    "P, I, D = 1, 0, 1\n",
    "tau = 0.02\n",
    "\n",
    "for i_episode in range(5):\n",
    "    state = env.reset()\n",
    "    integral = 0\n",
    "    derivative = 0\n",
    "    prev_error = 0\n",
    "    for t in range(500):\n",
    "        env.render()\n",
    "        integral += state[2] * tau\n",
    "        derivative = state[3]\n",
    "        error = state[2]\n",
    "        res = I * integral + D * derivative + P * error\n",
    "        if res>0:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 0\n",
    "\n",
    "        state, reward, done, info = env.step(action)\n",
    "        if done:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            break\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5195cf21-1da7-466f-9826-3580b1b87289",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "Maybe this way can lead us to a good result, but it requires the selection of coefficients. These coefficients I tried are not bad, but there is often drift in the x-coordinate"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
